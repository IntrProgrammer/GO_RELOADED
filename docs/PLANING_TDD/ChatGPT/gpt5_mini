Below is an ordered set of small, incremental Agile tasks specifically written for an entry-level Go developer practicing Test-Driven Development (TDD). Each task begins with writing tests, states the precise implementation goal, and ends with how to validate that the task is complete. Tasks collectively implement the Text Formatter described in the reference documents (see Architecture_Type.md for the chosen hybrid FSM/Pipeline approach). Use go test for unit tests; keep tests small and focused.

High-level architecture note (reference): per Architecture_Type.md we will implement a hybrid architecture leaning toward an FSM for control of editing steps (READING → EVALUATE → EDITING → back to READING) while enabling pipeline-like chunk processing for large files/chunks for throughput. Tests first; then code.

Task 0 — Project bootstrap
- Test (TDD):
  - Create initial test file: formatter/init_test.go.
  - Write a trivial test TestRepositorySetup that expects package "formatter" to build and a function NewFormatter() to return a non-nil formatter.
- Implementation goal:
  - Initialize a Go module (go mod init github.com/you/textformatter).
  - Create package formatter with NewFormatter() stub returning a struct pointer.
  - Add Makefile or simple scripts: go test runs tests.
- Validation:
  - Run go test ./... — tests pass (the trivial TestRepositorySetup passes).
  - go vet/go fmt produce no build issues.

Task 1 — Tokenizer: split text into tokens (words, punctuation, tags, quotes)
- Test (TDD):
  - Write table-driven tests in tokenizer/tokenizer_test.go for a function Tokenize(input string) []Token.
  - Test cases: simple sentence "Hello, world!" -> tokens ["Hello", ",", "world", "!"], tag example "one (hex)" -> ["one","(hex)"], quoted "' hello '": ["'", "hello", "'"].
- Implementation goal:
  - Implement Tokenize to produce a slice of Token structs ({Type, Value, Pos}).
  - Token types: Word, Punctuation (groups allowed like "...", "!!"), Tag (parentheses contents), Quote (single quote).
  - Keep tokenizer deterministic and well-documented.
- Validation:
  - go test passes all tokenizer tests.
  - Tokens preserve original order and exact punctuation groupings.

Task 2 — Punctuation normalization stage
- Test (TDD):
  - Write tests for function NormalizePunctuation(tokens []Token) []Token.
  - Cases:
    - "there ,and" -> attach comma to previous word and ensure single space after: tokens should represent "there," "and".
    - "thinking ... You" -> "thinking..." "You".
    - Multiple punctuation without prior space: "BAMM !!" -> "BAMM!!".
- Implementation goal:
  - Implement NormalizePunctuation to:
    - Merge punctuation tokens with the immediate previous word token.
    - Ensure token stream modeling yields a single whitespace after punctuation when rendered.
  - Keep transformation at token level; do not modify characters inside words.
- Validation:
  - go test passes punctuation tests.
  - Re-rendered text from tokens matches expected spacing for punctuation.

Task 3 — Single-quote inner-spacing cleaner
- Test (TDD):
  - Write tests for CleanSingleQuotes(tokens []Token) []Token or similar to remove extra spaces inside single quotes.
  - Example: " ' I am the ' " => produce tokens representing "'I am the'".
  - Ensure nested rules still apply (i.e., cleaning doesn't remove quotes).
- Implementation goal:
  - Adjust tokens inside pairs of single quotes: strip leading spaces after opening quote token and trailing spaces before closing quote token.
  - Preserve other tokens inside quotes for subsequent processing.
- Validation:
  - go test passes quote-cleaning tests.
  - Rendering tokens produces no spaces immediately after opening single quote or immediately before closing single quote.

Task 4 — Simple renderer to recompose tokens into string
- Test (TDD):
  - Tests for Render(tokens []Token) string:
    - Render should produce properly spaced strings according to punctuation rules and quotes from earlier steps.
    - Use results from Tasks 1–3 to assert round-trip behavior for given inputs.
- Implementation goal:
  - Implement Render to convert token slice back to a text string per spacing rules (no space before punctuation, exactly one space after punctuation/group, space between words, proper handling of quotes).
- Validation:
  - go test passes render tests.
  - Integration tests: Tokenize -> NormalizePunctuation -> CleanSingleQuotes -> Render yields expected final string for sample inputs.

Task 5 — Tag parser: identify tag type and optional number
- Test (TDD):
  - Tests for ParseTag(tagToken Token) Tag struct with fields Type string, Count int (default 1), and Raw string.
  - Cases: "(hex)", "(up, 2)", "(low)", "(cap,3)".
  - Validate parsing errors on malformed tags return error.
- Implementation goal:
  - Implement ParseTag to trim parentheses, split on comma for optional numbers, validate tag names (hex, bin, up, low, cap).
- Validation:
  - go test passes tag parsing tests.
  - Malformed tags tested: "(unknown)" returns parse error.

Task 6 — Number conversion rule: (hex) and (bin)
- Test (TDD):
  - Tests for ApplyNumberTag(tokens []Token, indexOfTag int) that converts the immediate previous Word token to its decimal representation.
  - Cases:
    - "1E (hex) files" => "30 files".
    - "10 (bin) years" => "2 years".
  - Tests for invalid number: "ZZ (hex)" should either leave original word and return an explicit error or tag an error token (decide behavior in test).
- Implementation goal:
  - Implement ApplyNumberTag that:
    - Locates the correct previous token (skip punctuation tokens when appropriate).
    - Parses word value using strconv.ParseInt with base 16 or 2.
    - Replaces previous token value with decimal string and removes tag token.
    - For invalid parse, return an error result; design choice: convert nothing and record an error state.
- Validation:
  - go test passes number conversion tests.
  - Integration tests combining with renderer confirm final string replacement.

Task 7 — Case-change single-word tags: (up), (low), (cap)
- Test (TDD):
  - Tests for ApplyCaseTag(tokens []Token, indexOfTag int) modifying previous single word.
  - Cases:
    - "go (up)!" => "GO!"
    - "SHOUTING (low)" => "shouting".
    - "brooklyn (cap)" => "Brooklyn".
- Implementation goal:
  - Implement ApplyCaseTag to change the previous word token's value according to tag type and remove the tag token.
  - Handle punctuation attached to word tokens (strip punctuation when changing case or preserve punctuation).
- Validation:
  - go test passes case-change single-word tests.
  - Rendered output shows case changes correctly attached to punctuation.

Task 8 — Multi-word case tags: (up, N), (low, N), (cap, N)
- Test (TDD):
  - Tests for multi-word count behavior for ApplyCaseTag with Count > 1.
  - Cases:
    - "This is so exciting (up, 2)" => "This is SO EXCITING".
    - "THE QUICK BROWN (low, 3)" => "the quick brown".
    - "a nice little bridge (cap, 3)" => "a Nice Little Bridge" (capitalization of each of the 3 previous words for cap? -- specify expected: cap means each word capitalized or first word? Define in test: cap makes each selected word Title Case).
- Implementation goal:
  - Implement multi-word selection: find N previous word tokens (skip punctuation and quotes tokens unless they are part of words) and apply case transform to each.
  - Remove tag token after application.
  - Decide and document cap semantics (prefer Title case for each word).
- Validation:
  - go test passes multi-word case tests.
  - Edge case tests: count > available words should apply to as many as exist without panic.

Task 9 — Tag application orchestration for a token sequence
- Test (TDD):
  - Table-driven tests that run ApplyAllTags(tokens []Token) and check full transformations for mixed-tag inputs.
  - Cases:
    - "1E (hex) and READY (low) and This is (up, 2)" -> expected final string after all tags are applied.
  - Test that tags are applied left-to-right or according to defined FSM evaluation order (decide in tests).
- Implementation goal:
  - Implement ApplyAllTags to iterate tokens in a deterministic order and apply ParseTag + appropriate Apply* function.
  - Ensure tags removed and token indices updated safely.
  - Choose conflict resolution rule (e.g., tags apply to the nearest previous words at time of application).
- Validation:
  - go test passes orchestration tests.
  - Sample inputs produce expected outputs.

Task 10 — "a" -> "an" rule
- Test (TDD):
  - Tests for ApplyIndefiniteArticleRule(tokens []Token) that replace "a" to "an" when next word begins with [a,e,i,o,u,h] (case-insensitive).
  - Cases:
    - "A amazing rock" => "An amazing rock"
    - "a house" => "an house"
    - "a user" => decide: user starts with 'u' but pronounced 'y' — out of scope; tests based on simple letter rule as specified (vowel or 'h').
- Implementation goal:
  - Implement ApplyIndefiniteArticleRule scanning tokens and replacing lone "a" (word token) with "an" if next word token starts with [aeiouhAEIOUH].
  - Ensure not to change "a." at end of sentence incorrectly (attach punctuation rules handled elsewhere).
- Validation:
  - go test passes article rule tests.
  - Integration tests show article correction alongside punctuation rules.

Task 11 — Rule order integration tests (inside single quote)
- Test (TDD):
  - Tests that rules apply to text inside single quotes and that all rules (tags, punctuation, a->an) apply inside quotes.
  - Example: "' I love 1E (hex) , a apple (cap) '" -> expected inside-quote formatting applied.
- Implementation goal:
  - Ensure ApplyAllRules pipeline treats quoted regions as tokens where the same transforms will be applied.
  - Integrate previous functions so they can be applied scoped by quote boundaries.
- Validation:
  - go test passes these integration tests where quoted content receives all rule transformations.

Task 12 — Token-based FSM core controller
- Test (TDD):
  - Tests for an FSM orchestrator struct (type FSM struct{}) that exposes ProcessTokens(tokens []Token) ([]Token, error) and follows states: READING → EVALUATE → EDITING → READING.
  - Tests assert state transitions occur and that ProcessTokens yields expected tokens for simple flows.
- Implementation goal:
  - Implement minimal FSM with explicit states and methods to run evaluation (detect tags), perform editing (apply rules), handle errors (enter ERROR state), and resume.
  - Define a small state machine interface to make unit testing states possible.
- Validation:
  - go test passes FSM tests verifying transitions and correct final token stream for sample inputs.

Task 13 — Chunked pipeline for large files
- Test (TDD):
  - Tests for a chunking function ChunkText(input string, maxTokens int) [][]Token and for PipelineProcess(chunks [][]Token) []Token.
  - Cases:
    - Large text split into chunks and recombined yields same output as single-pass processing (with tags that do not span chunk boundaries).
    - Ensure multi-word tags that might target words across chunk boundaries are handled per design (tests expect either strategy: either process chunk boundaries carefully or reject multi-word tags spanning chunks — decide and codify in test).
- Implementation goal:
  - Implement chunking logic and a pipeline that processes chunks concurrently (goroutines) then reassembles.
  - For the hybrid design: prefer FSM for small texts (single chunk) and pipeline for many chunks; make both available and testable.
- Validation:
  - go test passes chunking and pipeline tests.
  - Concurrency tests ensure no data races (run with go test -race).

Task 14 — File I/O and CLI entry point
- Test (TDD):
  - Tests for functions ReadFile(path string) (string, error) and WriteFile(path string, content string) error — use temp files.
  - CLI integration test: run main in test mode to process tmp input file and produce expected output file.
- Implementation goal:
  - Implement command-line tool (cmd/textformatter/main.go) that accepts input file, output file, and flags (e.g., --mode=fsm|pipeline, --chunk-size).
  - Use the formatter core to process file contents.
- Validation:
  - go test passes file I/O tests.
  - Manual run: ./textformatter -i in.txt -o out.txt produces expected output for sample inputs.

Task 15 — Error handling and graceful recovery states (FSM)
- Test (TDD):
  - Tests for how FSM handles parse/convert errors: e.g., an invalid hex value should cause the FSM to enter an ERROR state and produce a controlled error message or fallback behavior.
  - Tests check that the program doesn't panic and returns precise error details.
- Implementation goal:
  - Implement ERROR state behavior in FSM: log the problem, optionally skip offending tag, and continue or fail based on policy.
  - Provide configuration flag to choose fail-fast vs best-effort.
- Validation:
  - go test passes error handling tests.
  - Integration tests running with --policy=best-effort continue processing with errors reported.

Task 16 — Logging and diagnostic output
- Test (TDD):
  - Tests for a logger interface and tests that important events (tag application, conversions, FSM state transitions) call logger with expected messages (use a test logger capturing messages).
- Implementation goal:
  - Implement small logging abstraction so unit tests can inject a mock logger.
  - Emit informative events without cluttering core logic.
- Validation:
  - go test passes logger tests.
  - Running tool with --verbose outputs meaningful diagnostics.

Task 17 — Property and edge-case tests (fuzz-like)
- Test (TDD):
  - Create tests that feed randomized inputs and assert invariants:
    - Render(Tokenize(input)) does not panic.
    - Number of tokens after normalization is <= original tokens + small delta.
    - Quotes are balanced or reports an error.
  - Use table-based generated cases or use go-fuzz or go test with randomized generator.
- Implementation goal:
  - Add property-style tests to cover unusual inputs, Unicode, empty inputs, only punctuation, consecutive tags, tags with no preceding word.
  - Fix detected issues.
- Validation:
  - go test passes property tests.
  - No panics or races under randomized test runs.

Task 18 — Integration system tests and CI
- Test (TDD):
  - Add end-to-end tests in testdata/ that run the compiled binary against set of sample inputs (small, medium, large) and assert outputs match expected files.
  - Write tests to run go test ./... with -race and check coverage baseline.
- Implementation goal:
  - Add GitHub Actions (or other CI) workflow to run go test, go vet, go fmt, go vet, and static checks.
  - Ensure coverage threshold (e.g., > 80%) for business logic packages.
- Validation:
  - CI pipeline passes for repository.
  - All end-to-end tests pass in CI and locally.

Task 19 — Documentation, examples, and final polish
- Test (TDD):
  - Add tests that verify examples in README (e.g., example inputs produce expected outputs).
  - Tests ensure example code snippets compile (use examples_test.go).
- Implementation goal:
  - Write README explaining architecture choice (reference Architecture_Type.md stating hybrid-FSM leaning), CLI usage, configuration, and extension points.
  - Provide example inputs and expected outputs in testdata/.
- Validation:
  - go test passes example tests.
  - README and inline docs are clear; project builds cleanly.

Optional Task A — Use AI Agent to generate extra test cases
- Test (TDD):
  - Add tests that import generated test vectors (e.g., tests/ai_generated_cases_test.go) — treat these like normal table-driven tests.
- Implementation goal:
  - Use an AI agent to generate diverse real-world sentences and edge cases (export as JSON) and convert into unit tests.
- Validation:
  - go test passes those generated test cases (or highlights failing cases to be fixed).

Notes on incremental approach and expected workflow:
- Keep each task small and ensure one git commit per task.
- Each task explicitly starts by writing tests that fail, then implements minimal code to pass tests (TDD).
- Use table-driven tests in Go and small helper functions to avoid duplication.
- Run go test -race on concurrency tasks.
- For FSM vs Pipeline decisions, document in code and README how the hybrid decision is made (e.g., if tokens <= threshold use FSM sequential flow; otherwise chunk and use pipeline stages concurrently).
- Keep functions pure and side-effect-free where possible to make testing easier.

If you want, I can:
- Generate starter test skeletons and file structure for the first few tasks.
- Produce example Token and Tag struct definitions and minimal interfaces for FSM and pipeline to jump-start development. Which would you like me to generate next?
